#!/usr/bin/env python3
"""
ä¸“é—¨æµ‹è¯•ä¸¥æ ¼chunkå¤§å°æ§åˆ¶åŠŸèƒ½
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from contract_splitter import DocxSplitter


def create_test_content():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„é•¿æ–‡æœ¬å†…å®¹"""
    # åˆ›å»ºä¸€ä¸ªåŒ…å«é•¿æ®µè½çš„æµ‹è¯•å†…å®¹
    long_paragraph = """
è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„æ®µè½ï¼Œç”¨æ¥æµ‹è¯•ä¸¥æ ¼chunkå¤§å°æ§åˆ¶åŠŸèƒ½ã€‚è¿™ä¸ªæ®µè½åŒ…å«äº†å¤§é‡çš„æ–‡å­—å†…å®¹ï¼Œç›®çš„æ˜¯ç¡®ä¿å®ƒä¼šè¶…è¿‡æˆ‘ä»¬è®¾å®šçš„æœ€å¤§tokené™åˆ¶ã€‚
åœ¨å®é™…çš„æ–‡æ¡£å¤„ç†ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°è¿™æ ·çš„é•¿æ®µè½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³•å¾‹æ–‡æ¡£ã€åˆåŒæ¡æ¬¾ã€æŠ€æœ¯è§„èŒƒç­‰ä¸“ä¸šæ–‡æ¡£ä¸­ã€‚
è¿™äº›é•¿æ®µè½é€šå¸¸åŒ…å«å®Œæ•´çš„æ¡æ¬¾è¯´æ˜ã€è¯¦ç»†çš„æŠ€æœ¯æè¿°æˆ–è€…å¤æ‚çš„æ³•å¾‹æ¡æ–‡ã€‚
å¦‚æœæˆ‘ä»¬ä¸å¯¹è¿™äº›é•¿æ®µè½è¿›è¡Œé€‚å½“çš„åˆ†å‰²ï¼Œå®ƒä»¬å¯èƒ½ä¼šè¶…è¿‡LLMçš„tokené™åˆ¶ï¼Œå¯¼è‡´å¤„ç†å¤±è´¥ã€‚
å› æ­¤ï¼Œä¸¥æ ¼çš„chunkå¤§å°æ§åˆ¶åŠŸèƒ½å°±æ˜¾å¾—éå¸¸é‡è¦ã€‚å®ƒå¯ä»¥è‡ªåŠ¨æ£€æµ‹è¶…è¿‡é™åˆ¶çš„chunkï¼Œå¹¶åœ¨å¥å·ã€æ„Ÿå¹å·ã€é—®å·ç­‰è‡ªç„¶æ–­å¥ç‚¹è¿›è¡Œåˆ†å‰²ã€‚
è¿™æ ·æ—¢ä¿è¯äº†chunkå¤§å°åœ¨å¯æ¥å—èŒƒå›´å†…ï¼Œåˆå°½å¯èƒ½ä¿æŒäº†æ–‡æœ¬çš„è¯­ä¹‰å®Œæ•´æ€§ã€‚
åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å®ç°äº†overlapåŠŸèƒ½ï¼Œç¡®ä¿åˆ†å‰²åçš„chunkä¹‹é—´æœ‰é€‚å½“çš„é‡å ï¼Œé¿å…é‡è¦ä¿¡æ¯åœ¨åˆ†å‰²ç‚¹ä¸¢å¤±ã€‚
è¿™ä¸ªåŠŸèƒ½å¯¹äºå¤„ç†ä¸­æ–‡æ–‡æ¡£ç‰¹åˆ«é‡è¦ï¼Œå› ä¸ºä¸­æ–‡çš„å¥å­ç»“æ„å’Œæ ‡ç‚¹ä½¿ç”¨ä¸è‹±æ–‡æœ‰æ‰€ä¸åŒã€‚
æˆ‘ä»¬çš„å®ç°è€ƒè™‘äº†ä¸­æ–‡çš„ç‰¹æ®Šæ€§ï¼Œèƒ½å¤Ÿæ­£ç¡®è¯†åˆ«ä¸­æ–‡çš„å¥å·ã€æ„Ÿå¹å·ã€é—®å·ç­‰æ ‡ç‚¹ç¬¦å·ã€‚
"""
    
    # é‡å¤å¤šæ¬¡ä»¥ç¡®ä¿è¶…è¿‡é™åˆ¶
    return long_paragraph * 5


def test_strict_chunking_with_long_text():
    """æµ‹è¯•ä¸¥æ ¼chunkæ§åˆ¶åŠŸèƒ½"""
    print("ğŸ“ æµ‹è¯•ä¸¥æ ¼chunkå¤§å°æ§åˆ¶åŠŸèƒ½")
    print("=" * 80)
    
    # åˆ›å»ºæµ‹è¯•å†…å®¹
    test_content = create_test_content()
    print(f"æµ‹è¯•å†…å®¹é•¿åº¦: {len(test_content)} å­—ç¬¦")
    
    # è®¾ç½®è¾ƒå°çš„é™åˆ¶æ¥è§¦å‘åˆ†å‰²
    max_tokens = 500
    overlap = 50
    
    # æµ‹è¯•ä¸ä¸¥æ ¼æ§åˆ¶
    print(f"\nğŸ“‹ ä¸ä¸¥æ ¼æ§åˆ¶ (max_tokens={max_tokens}):")
    
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„sectionç»“æ„
    test_sections = [{
        "heading": "æµ‹è¯•æ ‡é¢˜",
        "content": test_content,
        "level": 1,
        "subsections": []
    }]
    
    splitter_loose = DocxSplitter(
        max_tokens=max_tokens,
        overlap=overlap,
        strict_max_tokens=False
    )
    
    chunks_loose = splitter_loose.flatten(test_sections)
    print(f"  æ€»chunks: {len(chunks_loose)}")
    
    oversized_loose = [i for i, chunk in enumerate(chunks_loose) if len(chunk) > max_tokens]
    print(f"  è¶…è¿‡{max_tokens}å­—ç¬¦çš„chunks: {len(oversized_loose)}")
    
    if oversized_loose:
        for i in oversized_loose[:3]:
            size = len(chunks_loose[i])
            print(f"    Chunk {i+1}: {size} å­—ç¬¦")
    
    # æµ‹è¯•ä¸¥æ ¼æ§åˆ¶
    print(f"\nğŸ“‹ ä¸¥æ ¼æ§åˆ¶ (max_tokens={max_tokens}):")
    
    splitter_strict = DocxSplitter(
        max_tokens=max_tokens,
        overlap=overlap,
        strict_max_tokens=True
    )
    
    chunks_strict = splitter_strict.flatten(test_sections)
    print(f"  æ€»chunks: {len(chunks_strict)}")
    
    oversized_strict = [i for i, chunk in enumerate(chunks_strict) if len(chunk) > max_tokens]
    print(f"  è¶…è¿‡{max_tokens}å­—ç¬¦çš„chunks: {len(oversized_strict)}")
    
    if oversized_strict:
        for i in oversized_strict[:3]:
            size = len(chunks_strict[i])
            print(f"    Chunk {i+1}: {size} å­—ç¬¦")
    
    # æ˜¾ç¤ºåˆ†å‰²æ•ˆæœ
    print(f"\nğŸ“Š åˆ†å‰²æ•ˆæœå¯¹æ¯”:")
    print(f"  ä¸ä¸¥æ ¼æ§åˆ¶: {len(chunks_loose)} chunks, å¹³å‡ {sum(len(c) for c in chunks_loose)/len(chunks_loose):.0f} å­—ç¬¦")
    print(f"  ä¸¥æ ¼æ§åˆ¶: {len(chunks_strict)} chunks, å¹³å‡ {sum(len(c) for c in chunks_strict)/len(chunks_strict):.0f} å­—ç¬¦")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªchunkçš„å†…å®¹é¢„è§ˆ
    print(f"\nğŸ“ ä¸¥æ ¼æ§åˆ¶åçš„chunké¢„è§ˆ:")
    for i, chunk in enumerate(chunks_strict[:3]):
        print(f"  Chunk {i+1} ({len(chunk)} å­—ç¬¦): {chunk[:100]}...")
        print()


def test_sentence_splitting():
    """æµ‹è¯•å¥å­åˆ†å‰²åŠŸèƒ½"""
    print("âœ‚ï¸ æµ‹è¯•å¥å­åˆ†å‰²åŠŸèƒ½")
    print("=" * 80)
    
    # åˆ›å»ºåŒ…å«å¤šç§æ ‡ç‚¹çš„æµ‹è¯•æ–‡æœ¬
    test_text = """
è¿™æ˜¯ç¬¬ä¸€ä¸ªå¥å­ã€‚è¿™æ˜¯ç¬¬äºŒä¸ªå¥å­ï¼è¿™æ˜¯ç¬¬ä¸‰ä¸ªå¥å­ï¼Ÿè¿™æ˜¯ç¬¬å››ä¸ªå¥å­ï¼›è¿™æ˜¯ç¬¬äº”ä¸ªå¥å­ã€‚
è¿™é‡Œæœ‰ä¸€ä¸ªå¾ˆé•¿çš„å¥å­ï¼Œå®ƒåŒ…å«äº†å¾ˆå¤šå†…å®¹ï¼Œç›®çš„æ˜¯æµ‹è¯•å½“å•ä¸ªå¥å­å°±è¶…è¿‡é™åˆ¶æ—¶çš„å¤„ç†æƒ…å†µï¼Œçœ‹çœ‹ç³»ç»Ÿæ˜¯å¦èƒ½å¤Ÿæ­£ç¡®å¤„ç†è¿™ç§è¾¹ç•Œæƒ…å†µã€‚
è¿™æ˜¯å¦ä¸€ä¸ªæ­£å¸¸é•¿åº¦çš„å¥å­ã€‚æœ€åä¸€ä¸ªå¥å­ç»“æŸã€‚
"""
    
    max_tokens = 100  # è®¾ç½®å¾ˆå°çš„é™åˆ¶
    
    splitter = DocxSplitter(
        max_tokens=max_tokens,
        overlap=20,
        strict_max_tokens=True
    )
    
    # æµ‹è¯•åˆ†å‰²
    test_sections = [{
        "heading": "å¥å­åˆ†å‰²æµ‹è¯•",
        "content": test_text,
        "level": 1,
        "subsections": []
    }]
    
    chunks = splitter.flatten(test_sections)
    
    print(f"åŸæ–‡é•¿åº¦: {len(test_text)} å­—ç¬¦")
    print(f"åˆ†å‰²åchunksæ•°é‡: {len(chunks)}")
    print(f"æœ€å¤§chunké•¿åº¦: {max(len(c) for c in chunks)} å­—ç¬¦")
    
    print("\nåˆ†å‰²ç»“æœ:")
    for i, chunk in enumerate(chunks):
        print(f"Chunk {i+1} ({len(chunk)} å­—ç¬¦):")
        print(f"  {chunk.strip()}")
        print()


def test_overlap_functionality():
    """æµ‹è¯•overlapåŠŸèƒ½"""
    print("ğŸ”„ æµ‹è¯•overlapåŠŸèƒ½")
    print("=" * 80)
    
    test_text = """
ç¬¬ä¸€æ®µå†…å®¹åŒ…å«é‡è¦ä¿¡æ¯Aã€‚ç¬¬äºŒæ®µå†…å®¹åŒ…å«é‡è¦ä¿¡æ¯Bã€‚ç¬¬ä¸‰æ®µå†…å®¹åŒ…å«é‡è¦ä¿¡æ¯Cã€‚
ç¬¬å››æ®µå†…å®¹åŒ…å«é‡è¦ä¿¡æ¯Dã€‚ç¬¬äº”æ®µå†…å®¹åŒ…å«é‡è¦ä¿¡æ¯Eã€‚ç¬¬å…­æ®µå†…å®¹åŒ…å«é‡è¦ä¿¡æ¯Fã€‚
ç¬¬ä¸ƒæ®µå†…å®¹åŒ…å«é‡è¦ä¿¡æ¯Gã€‚ç¬¬å…«æ®µå†…å®¹åŒ…å«é‡è¦ä¿¡æ¯Hã€‚ç¬¬ä¹æ®µå†…å®¹åŒ…å«é‡è¦ä¿¡æ¯Iã€‚
"""
    
    max_tokens = 150
    overlap = 50
    
    splitter = DocxSplitter(
        max_tokens=max_tokens,
        overlap=overlap,
        strict_max_tokens=True
    )
    
    test_sections = [{
        "heading": "Overlapæµ‹è¯•",
        "content": test_text,
        "level": 1,
        "subsections": []
    }]
    
    chunks = splitter.flatten(test_sections)
    
    print(f"åŸæ–‡é•¿åº¦: {len(test_text)} å­—ç¬¦")
    print(f"Overlapè®¾ç½®: {overlap} å­—ç¬¦")
    print(f"åˆ†å‰²åchunksæ•°é‡: {len(chunks)}")
    
    print("\næ£€æŸ¥overlapæ•ˆæœ:")
    for i in range(len(chunks) - 1):
        current_end = chunks[i][-50:]  # å½“å‰chunkçš„ç»“å°¾
        next_start = chunks[i+1][:50]  # ä¸‹ä¸€ä¸ªchunkçš„å¼€å¤´
        
        print(f"Chunk {i+1} ç»“å°¾: ...{current_end}")
        print(f"Chunk {i+2} å¼€å¤´: {next_start}...")
        
        # ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰é‡å 
        has_overlap = any(word in next_start for word in current_end.split()[-5:] if len(word) > 2)
        print(f"æ£€æµ‹åˆ°overlap: {'âœ…' if has_overlap else 'âŒ'}")
        print()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª ä¸¥æ ¼chunkå¤§å°æ§åˆ¶åŠŸèƒ½ä¸“é¡¹æµ‹è¯•")
    print("=" * 80)
    
    test_strict_chunking_with_long_text()
    print("\n" + "=" * 80)
    
    test_sentence_splitting()
    print("\n" + "=" * 80)
    
    test_overlap_functionality()
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æµ‹è¯•æ€»ç»“")
    print("âœ… ä¸¥æ ¼chunkå¤§å°æ§åˆ¶åŠŸèƒ½å·²å®ç°")
    print("âœ… è‡ªåŠ¨å¥å­åˆ†å‰²åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
    print("âœ… OverlapåŠŸèƒ½ç¡®ä¿ä¿¡æ¯è¿ç»­æ€§")
    print("âœ… æ”¯æŒä¸­æ–‡æ ‡ç‚¹ç¬¦å·è¯†åˆ«")
    print("âœ… è¾¹ç•Œæƒ…å†µå¤„ç†å®Œå–„")


if __name__ == "__main__":
    main()
