#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†outputç›®å½•ä¸‹çš„åˆ†ç±»æ–‡ä»¶
æ ¹æ®ä¸åŒå­ç›®å½•é‡‡ç”¨é€‚å½“çš„helperå‡½æ•°è¿›è¡Œæ‹†åˆ†æµ‹è¯•
"""

import os
import sys
import json
import shutil
from pathlib import Path
from typing import List, Dict, Any

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from contract_splitter.splitter_factory import SplitterFactory, get_default_factory
from contract_splitter.domain_helpers import (
    split_legal_document,
    split_contract,
    split_regulation,
    LegalClauseSplitter,
    DomainContractSplitter,
    RegulationSplitter
)


# WPSå’ŒPDFå¤„ç†ç°åœ¨é›†æˆåˆ°DocxSplitterä¸­ï¼Œä¸éœ€è¦å•ç‹¬çš„è½¬æ¢å‡½æ•°


def get_directory_helper_config(directory_name: str) -> Dict[str, Any]:
    """
    æ ¹æ®ç›®å½•åç§°è·å–ç›¸åº”çš„helperé…ç½®
    
    Args:
        directory_name: ç›®å½•åç§°
        
    Returns:
        é…ç½®å­—å…¸
    """
    configs = {
        "contract": {
            "helper_type": "contract",
            "contract_type": "general",
            "description": "åˆåŒæ–‡ä»¶"
        },
        "law": {
            "helper_type": "legal",
            "description": "æ³•å¾‹æ³•è§„æ–‡ä»¶"
        },
        "rule": {
            "helper_type": "regulation",
            "regulation_type": "general",
            "description": "è§„ç« åˆ¶åº¦æ–‡ä»¶"
        },
        "others": {
            "helper_type": "general",
            "description": "å…¶ä»–æ–‡ä»¶"
        }
    }
    
    return configs.get(directory_name, configs["others"])


def save_chunks_to_file(file_path: str, chunks: List[str], output_dir: str = "output/chunks") -> str:
    """
    å°†chunksä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶ä¸­

    Args:
        file_path: åŸæ–‡ä»¶è·¯å¾„
        chunks: chunksåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    from datetime import datetime

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    file_name = os.path.basename(file_path)
    base_name = os.path.splitext(file_name)[0]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(output_dir, f"{base_name}_chunks_{timestamp}.txt")

    # å†™å…¥chunks
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"æ–‡æ¡£: {file_name}\n")
        f.write(f"åŸæ–‡ä»¶è·¯å¾„: {file_path}\n")
        f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»chunksæ•°: {len(chunks)}\n")
        f.write("=" * 80 + "\n\n")

        for i, chunk in enumerate(chunks, 1):
            f.write(f"Chunk {i}:\n")
            f.write("-" * 40 + "\n")
            f.write(chunk)
            f.write("\n\n" + "=" * 80 + "\n\n")

    return output_file


def process_single_file(file_path: str, config: Dict[str, Any], max_tokens: int = 3000, output_dir: str = "output/chunks") -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        config: å¤„ç†é…ç½®
        max_tokens: æœ€å¤§tokenæ•°
        
    Returns:
        å¤„ç†ç»“æœ
    """
    result = {
        "file_path": file_path,
        "success": False,
        "chunks_count": 0,
        "error": None,
        "output_files": []
    }
    
    try:
        # ä½¿ç”¨å·¥å‚æ¨¡å¼æ£€æŸ¥æ–‡ä»¶æ”¯æŒ
        factory = get_default_factory()
        file_info = factory.get_file_info(file_path)

        if not file_info['supported']:
            result["error"] = f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_info['format']}"
            return result

        print(f"ğŸ”„ å¤„ç†{file_info['format'].upper()}æ–‡ä»¶: {file_path}")
        print(f"   ä½¿ç”¨å¤„ç†å™¨: {file_info['splitter_class']}")
        
        # æ ¹æ®é…ç½®é€‰æ‹©å¤„ç†æ–¹æ³•
        helper_type = config.get("helper_type", "general")
        
        if helper_type == "legal":
            print(f"âš–ï¸ ä½¿ç”¨æ³•å¾‹æ¡æ¬¾åˆ‡åˆ†å™¨å¤„ç†: {file_path}")
            chunks = split_legal_document(
                file_path,
                max_tokens=max_tokens,
                strict_max_tokens=True
            )

        elif helper_type == "contract":
            contract_type = config.get("contract_type", "general")
            print(f"ğŸ“„ ä½¿ç”¨åˆåŒåˆ‡åˆ†å™¨å¤„ç† ({contract_type}): {file_path}")
            chunks = split_contract(
                file_path,
                contract_type=contract_type,
                max_tokens=max_tokens,
                strict_max_tokens=True
            )

        elif helper_type == "regulation":
            regulation_type = config.get("regulation_type", "general")
            print(f"ğŸ“‹ ä½¿ç”¨è§„ç« åˆ¶åº¦åˆ‡åˆ†å™¨å¤„ç† ({regulation_type}): {file_path}")
            chunks = split_regulation(
                file_path,
                regulation_type=regulation_type,
                max_tokens=max_tokens,
                strict_max_tokens=True
            )

        else:
            print(f"ğŸ“„ ä½¿ç”¨å·¥å‚æ¨¡å¼è‡ªåŠ¨é€‰æ‹©å¤„ç†å™¨: {file_path}")
            # ä½¿ç”¨å·¥å‚æ¨¡å¼è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„splitter
            chunks = factory.split_and_flatten(
                file_path,
                max_tokens=max_tokens,
                strict_max_tokens=True
            )
        
        # ä¿å­˜æ‰€æœ‰chunksåˆ°å•ä¸ªæ–‡ä»¶
        if chunks:
            output_file = save_chunks_to_file(file_path, chunks, output_dir)
            result["output_files"].append(output_file)
        
        result["success"] = True
        result["chunks_count"] = len(chunks)
        
        print(f"âœ… å¤„ç†æˆåŠŸ: {len(chunks)} ä¸ªchunks")
        
    except Exception as e:
        result["error"] = str(e)
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
    
    return result


def scan_and_process_directory(base_dir: str = "output", max_tokens: int = 3000, output_dir: str = "output/chunks") -> Dict[str, Any]:
    """
    æ‰«æå¹¶å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    
    Args:
        base_dir: åŸºç¡€ç›®å½•
        max_tokens: æœ€å¤§tokenæ•°
        
    Returns:
        å¤„ç†ç»“æœæ±‡æ€»
    """
    results = {
        "total_files": 0,
        "successful_files": 0,
        "failed_files": 0,
        "total_chunks": 0,
        "directory_results": {},
        "errors": []
    }
    
    base_path = Path(base_dir)
    
    if not base_path.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return results
    
    # æ‰«ææ‰€æœ‰å­ç›®å½•
    for subdir in base_path.iterdir():
        if not subdir.is_dir():
            continue
        
        subdir_name = subdir.name
        config = get_directory_helper_config(subdir_name)
        
        print(f"\nğŸ“ å¤„ç†ç›®å½•: {subdir_name} ({config['description']})")
        print("=" * 80)
        
        dir_results = {
            "config": config,
            "files": [],
            "total_files": 0,
            "successful_files": 0,
            "total_chunks": 0
        }
        
        # å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for file_path in subdir.iterdir():
            if file_path.is_file():
                file_ext = file_path.suffix.lower()
                
                # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
                if file_ext in ['.docx', '.doc', '.wps', '.pdf']:
                    print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
                    
                    file_result = process_single_file(str(file_path), config, max_tokens, output_dir)
                    dir_results["files"].append(file_result)
                    dir_results["total_files"] += 1
                    results["total_files"] += 1
                    
                    if file_result["success"]:
                        dir_results["successful_files"] += 1
                        dir_results["total_chunks"] += file_result["chunks_count"]
                        results["successful_files"] += 1
                        results["total_chunks"] += file_result["chunks_count"]
                    else:
                        results["failed_files"] += 1
                        results["errors"].append({
                            "file": str(file_path),
                            "error": file_result["error"]
                        })
        
        results["directory_results"][subdir_name] = dir_results
        
        print(f"\nğŸ“Š {subdir_name} ç›®å½•å¤„ç†å®Œæˆ:")
        print(f"  æ€»æ–‡ä»¶æ•°: {dir_results['total_files']}")
        print(f"  æˆåŠŸå¤„ç†: {dir_results['successful_files']}")
        print(f"  ç”Ÿæˆchunks: {dir_results['total_chunks']}")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ‰¹é‡æ–‡æ¡£å¤„ç†æµ‹è¯•")
    print("=" * 80)
    print("ğŸ“‹ é…ç½®:")
    print(f"  æœ€å¤§tokenæ•°: 3000")
    print(f"  ä¸¥æ ¼chunkæ§åˆ¶: å¯ç”¨")

    # æ˜¾ç¤ºå·¥å‚æ¨¡å¼æ”¯æŒçš„æ ¼å¼
    factory = get_default_factory()
    supported_formats = factory.get_supported_formats()
    print(f"  æ”¯æŒæ ¼å¼: {', '.join(supported_formats).upper()}")

    # æ˜¾ç¤ºæ ¼å¼èƒ½åŠ›
    capabilities = factory.get_format_capabilities()
    print("  å¤„ç†å™¨æ˜ å°„:")
    for format_type, info in capabilities.items():
        print(f"    {format_type.upper()}: {info['splitter_class']}")
    
    # å¼€å§‹å¤„ç†
    output_chunks_dir = "output/chunks"
    print(f"  è¾“å‡ºç›®å½•: {output_chunks_dir}")
    results = scan_and_process_directory("output", max_tokens=3000, output_dir=output_chunks_dir)
    
    # è¾“å‡ºæ±‡æ€»ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š å¤„ç†ç»“æœæ±‡æ€»")
    print("=" * 80)
    print(f"æ€»æ–‡ä»¶æ•°: {results['total_files']}")
    print(f"æˆåŠŸå¤„ç†: {results['successful_files']}")
    print(f"å¤„ç†å¤±è´¥: {results['failed_files']}")
    print(f"æ€»chunksæ•°: {results['total_chunks']}")
    
    # æŒ‰ç›®å½•æ˜¾ç¤ºç»“æœ
    for dir_name, dir_result in results["directory_results"].items():
        print(f"\nğŸ“ {dir_name} ({dir_result['config']['description']}):")
        print(f"  æ–‡ä»¶æ•°: {dir_result['total_files']}")
        print(f"  æˆåŠŸ: {dir_result['successful_files']}")
        print(f"  Chunks: {dir_result['total_chunks']}")
    
    # æ˜¾ç¤ºé”™è¯¯
    if results["errors"]:
        print(f"\nâŒ å¤„ç†å¤±è´¥çš„æ–‡ä»¶:")
        for error in results["errors"]:
            print(f"  {error['file']}: {error['error']}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSONæ–‡ä»¶
    output_file = "output/batch_processing_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    main()
