#!/usr/bin/env python3
"""
ç»¼åˆChunkæµ‹è¯•è„šæœ¬ - æ”¯æŒoutputç›®å½•ä¸‹çš„.doc/.docx/.pdfæ–‡ä»¶
"""
 
import sys
import os
import json
import glob
from pathlib import Path
from typing import List, Dict, Any
 
# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
 
from contract_splitter import ContractSplitter, split_document, flatten_sections
from contract_splitter.domain_helpers import split_legal_document, split_contract, split_regulation
from contract_splitter.utils import (
    count_tokens, split_chinese_sentences, sliding_window_split, clean_text
)
 
 
def find_document_files() -> List[str]:
    """æŸ¥æ‰¾outputç›®å½•ä¸‹çš„æ‰€æœ‰æ”¯æŒæ ¼å¼çš„æ–‡æ¡£"""
    supported_extensions = ['*.doc', '*.docx', '*.pdf', '*.wps']
    document_files = []

    for ext in supported_extensions:
        files = glob.glob(f"output/{ext}")
        document_files.extend(files)
        # ä¹Ÿæœç´¢å­ç›®å½•
        files = glob.glob(f"output/*/{ext}")
        document_files.extend(files)

    return sorted(document_files)


def find_wps_files() -> List[str]:
    """ä¸“é—¨æŸ¥æ‰¾WPSæ–‡ä»¶"""
    wps_files = []

    # æœç´¢outputç›®å½•åŠå…¶å­ç›®å½•ä¸‹çš„WPSæ–‡ä»¶
    wps_files.extend(glob.glob("output/*.wps"))
    wps_files.extend(glob.glob("output/*/*.wps"))
    wps_files.extend(glob.glob("output/*/*/*.wps"))

    return sorted(wps_files)


def is_legal_document(file_path: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºæ³•å¾‹æ–‡æ¡£"""
    file_path_lower = file_path.lower()

    # æ£€æŸ¥è·¯å¾„ä¸­æ˜¯å¦åŒ…å«æ³•å¾‹ç›¸å…³å…³é”®è¯
    legal_keywords = ['law', 'æ³•å¾‹', 'æ³•è§„', 'æ¡ä¾‹', 'åŠæ³•', 'è§„å®š', 'ç®¡ç†', 'ç›‘ç£', 'è¯åˆ¸', 'é“¶è¡Œ', 'é‡‘è']

    # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
    if '/law/' in file_path_lower or '\\law\\' in file_path_lower:
        return True

    # æ£€æŸ¥æ–‡ä»¶å
    file_name = Path(file_path).stem.lower()
    for keyword in legal_keywords:
        if keyword in file_name:
            return True

    return False
 
 
def test_file_chunking(file_path: str) -> Dict[str, Any]:
    """å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡Œchunkingæµ‹è¯•"""
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•æ–‡ä»¶: {file_path}")
    print(f"{'='*60}")
    
    result = {
        "file_path": file_path,
        "file_size": os.path.getsize(file_path),
        "file_extension": Path(file_path).suffix.lower(),
        "test_results": {}
    }
    
    try:
        # æµ‹è¯•ä¸åŒçš„chunkingç­–ç•¥
        strategies = [
            {"name": "small_chunks", "max_tokens": 500, "overlap": 50},
            {"name": "medium_chunks", "max_tokens": 1000, "overlap": 100},
            {"name": "large_chunks", "max_tokens": 2000, "overlap": 200}
        ]
        
        for strategy in strategies:
            print(f"\n--- ç­–ç•¥: {strategy['name']} (max_tokens={strategy['max_tokens']}, overlap={strategy['overlap']}) ---")
            
            strategy_result = {
                "max_tokens": strategy["max_tokens"],
                "overlap": strategy["overlap"],
                "hierarchical_sections": None,
                "flattened_chunks": None,
                "processing_time": None
            }
            
            import time
            start_time = time.time()
            
            try:
                # æ™ºèƒ½é€‰æ‹©å¤„ç†æ–¹æ³•
                if is_legal_document(file_path):
                    print(f"  ğŸ“š æ£€æµ‹åˆ°æ³•å¾‹æ–‡æ¡£ï¼Œä½¿ç”¨ä¸“ç”¨åˆ‡åˆ†å™¨")
                    # ä½¿ç”¨æ³•å¾‹æ–‡æ¡£ä¸“ç”¨åˆ‡åˆ†å™¨
                    chunks = split_legal_document(
                        file_path,
                        max_tokens=strategy["max_tokens"]
                    )

                    # æ¨¡æ‹Ÿsectionsç»“æ„ä»¥ä¿æŒå…¼å®¹æ€§
                    sections = [{"heading": f"æ³•å¾‹æ¡æ–‡ {i+1}", "content": chunk, "subsections": []}
                              for i, chunk in enumerate(chunks)]
                    flattened_chunks = chunks
                else:
                    print(f"  ğŸ“„ ä½¿ç”¨é€šç”¨åˆ‡åˆ†å™¨")
                    # ä½¿ç”¨ContractSplitterè¿›è¡Œå±‚æ¬¡åŒ–åˆ†å‰²
                    splitter = ContractSplitter(
                        max_tokens=strategy["max_tokens"],
                        overlap=strategy["overlap"],
                        split_by_sentence=True,
                        token_counter="character"
                    )

                    # è·å–å±‚æ¬¡åŒ–ç»“æ„
                    sections = splitter.split(file_path)
                    # å±•å¹³ä¸ºchunks
                    flattened_chunks = splitter.flatten(sections)

                strategy_result["hierarchical_sections"] = {
                    "num_sections": len(sections),
                    "sections_preview": []
                }
                
                # å±•ç¤ºå‰å‡ ä¸ªsectionsçš„ç»“æ„
                for i, section in enumerate(sections[:3]):
                    section_info = {
                        "section_id": i,
                        "heading": section.get("heading", "æ— æ ‡é¢˜"),
                        "level": section.get("level", 0),
                        "content_length": len(section.get("content", "")),
                        "content_preview": section.get("content", "")[:100] + "...",
                        "num_subsections": len(section.get("subsections", []))
                    }
                    strategy_result["hierarchical_sections"]["sections_preview"].append(section_info)
                
                print(f"âœ“ å±‚æ¬¡åŒ–åˆ†å‰²æˆåŠŸ: {len(sections)} ä¸ªsections")

                strategy_result["flattened_chunks"] = {
                    "num_chunks": len(flattened_chunks),
                    "chunks_preview": []
                }
                
                # å±•ç¤ºå‰å‡ ä¸ªchunks
                for i, chunk in enumerate(flattened_chunks[:3]):
                    chunk_info = {
                        "chunk_id": i,
                        "length": len(chunk),
                        "preview": chunk[:150] + "..." if len(chunk) > 150 else chunk
                    }
                    strategy_result["flattened_chunks"]["chunks_preview"].append(chunk_info)
                
                print(f"âœ“ å±•å¹³åˆ†å‰²æˆåŠŸ: {len(flattened_chunks)} ä¸ªchunks")
                
                # åˆ†æchunksè´¨é‡
                if flattened_chunks:
                    avg_chunk_length = sum(len(chunk) for chunk in flattened_chunks) / len(flattened_chunks)
                    max_chunk_length = max(len(chunk) for chunk in flattened_chunks)
                    min_chunk_length = min(len(chunk) for chunk in flattened_chunks)
                    
                    strategy_result["chunk_analysis"] = {
                        "avg_chunk_length": round(avg_chunk_length, 2),
                        "max_chunk_length": max_chunk_length,
                        "min_chunk_length": min_chunk_length,
                        "chunks_within_limit": sum(1 for chunk in flattened_chunks if len(chunk) <= strategy["max_tokens"])
                    }
                    
                    print(f"  å¹³å‡chunké•¿åº¦: {avg_chunk_length:.0f} å­—ç¬¦")
                    print(f"  æœ€å¤§chunké•¿åº¦: {max_chunk_length} å­—ç¬¦")
                    print(f"  æœ€å°chunké•¿åº¦: {min_chunk_length} å­—ç¬¦")
                    print(f"  ç¬¦åˆé•¿åº¦é™åˆ¶çš„chunks: {strategy_result['chunk_analysis']['chunks_within_limit']}/{len(flattened_chunks)}")
                
            except Exception as e:
                print(f"âœ— å¤„ç†å¤±è´¥: {e}")
                strategy_result["error"] = str(e)
            
            finally:
                strategy_result["processing_time"] = round(time.time() - start_time, 2)
                print(f"  å¤„ç†æ—¶é—´: {strategy_result['processing_time']}s")
            
            result["test_results"][strategy["name"]] = strategy_result
        
    except Exception as e:
        print(f"âœ— æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
        result["error"] = str(e)
    
    return result
 
 
def test_token_counting_comparison(file_path: str) -> Dict[str, Any]:
    """æµ‹è¯•ä¸åŒtokenè®¡æ•°æ–¹æ³•çš„å¯¹æ¯”"""
    print(f"\n{'='*60}")
    print(f"Tokenè®¡æ•°æ–¹æ³•å¯¹æ¯”: {file_path}")
    print(f"{'='*60}")
    
    result = {
        "file_path": file_path,
        "token_methods": {}
    }
    
    try:
        # è·å–æ–‡æ¡£å†…å®¹
        splitter = ContractSplitter()
        sections = splitter.split(file_path)
        flattened = splitter.flatten(sections)
        full_text = " ".join(flattened)
        
        # æµ‹è¯•ä¸åŒçš„tokenè®¡æ•°æ–¹æ³•
        methods = ["character"]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰tiktoken
        try:
            import tiktoken
            methods.append("tiktoken")
            print("âœ“ tiktokenå¯ç”¨")
        except ImportError:
            print("âœ— tiktokenä¸å¯ç”¨ï¼Œä»…ä½¿ç”¨å­—ç¬¦è®¡æ•°")
        
        for method in methods:
            print(f"\n--- {method} è®¡æ•°æ–¹æ³• ---")
            
            token_count = count_tokens(full_text, method)
            print(f"æ€»tokenæ•°: {token_count}")
            
            # æµ‹è¯•ä¸åŒchunkå¤§å°çš„æ•ˆæœ
            chunk_sizes = [500, 1000, 1500]
            method_result = {
                "total_tokens": token_count,
                "chunk_tests": {}
            }
            
            for chunk_size in chunk_sizes:
                chunks = sliding_window_split(
                    text=full_text,
                    max_tokens=chunk_size,
                    overlap=50,
                    by_sentence=True,
                    token_counter=method
                )
                
                method_result["chunk_tests"][f"size_{chunk_size}"] = {
                    "max_tokens": chunk_size,
                    "num_chunks": len(chunks),
                    "avg_tokens_per_chunk": round(sum(count_tokens(chunk, method) for chunk in chunks) / len(chunks), 2) if chunks else 0
                }
                
                print(f"  Chunkå¤§å° {chunk_size}: {len(chunks)} ä¸ªchunks")
            
            result["token_methods"][method] = method_result
        
    except Exception as e:
        print(f"âœ— Tokenè®¡æ•°æµ‹è¯•å¤±è´¥: {e}")
        result["error"] = str(e)
    
    return result


def test_chinese_text_processing(file_path: str) -> Dict[str, Any]:
    """æµ‹è¯•ä¸­æ–‡æ–‡æœ¬å¤„ç†èƒ½åŠ›"""
    print(f"\n{'='*60}")
    print(f"ä¸­æ–‡æ–‡æœ¬å¤„ç†æµ‹è¯•: {file_path}")
    print(f"{'='*60}")

    result = {
        "file_path": file_path,
        "chinese_processing": {}
    }

    try:
        # è·å–æ–‡æ¡£å†…å®¹
        splitter = ContractSplitter()
        sections = splitter.split(file_path)
        flattened = splitter.flatten(sections)

        # é€‰æ‹©ä¸€ä¸ªåŒ…å«ä¸­æ–‡çš„chunkè¿›è¡Œæµ‹è¯•
        chinese_chunk = None
        for chunk in flattened:
            if any('\u4e00' <= char <= '\u9fff' for char in chunk):
                chinese_chunk = chunk
                break

        if chinese_chunk:
            print(f"âœ“ æ‰¾åˆ°ä¸­æ–‡å†…å®¹ï¼Œé•¿åº¦: {len(chinese_chunk)} å­—ç¬¦")

            # æµ‹è¯•ä¸­æ–‡å¥å­åˆ†å‰²
            sentences = split_chinese_sentences(chinese_chunk[:500])  # å–å‰500å­—ç¬¦æµ‹è¯•
            print(f"âœ“ ä¸­æ–‡å¥å­åˆ†å‰²: {len(sentences)} ä¸ªå¥å­")

            # æµ‹è¯•æ–‡æœ¬æ¸…ç†
            cleaned_text = clean_text(chinese_chunk[:200])
            print(f"âœ“ æ–‡æœ¬æ¸…ç†å®Œæˆ")

            result["chinese_processing"] = {
                "has_chinese": True,
                "sample_length": len(chinese_chunk),
                "sentences_count": len(sentences),
                "sample_sentences": sentences[:3],  # å‰3ä¸ªå¥å­
                "cleaned_sample": cleaned_text[:100] + "..." if len(cleaned_text) > 100 else cleaned_text
            }
        else:
            print("âœ— æœªæ‰¾åˆ°ä¸­æ–‡å†…å®¹")
            result["chinese_processing"] = {
                "has_chinese": False,
                "message": "æ–‡æ¡£ä¸­æœªæ£€æµ‹åˆ°ä¸­æ–‡å†…å®¹"
            }

    except Exception as e:
        print(f"âœ— ä¸­æ–‡å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        result["error"] = str(e)

    return result


def save_individual_chunks(file_path: str, test_results: Dict[str, Any], output_dir: str = "output"):
    """ä¸ºå•ä¸ªæ–‡ä»¶ä¿å­˜chunksåˆ°ç‹¬ç«‹æ–‡ä»¶ï¼Œæ–¹ä¾¿äººå·¥æ ¸å¯¹"""
    file_name = Path(file_path).stem
    safe_name = "".join(c for c in file_name if c.isalnum() or c in (' ', '-', '_')).rstrip()

    # åˆ›å»ºæ–‡ä»¶ä¸“ç”¨ç›®å½•
    file_output_dir = Path(output_dir) / "individual_chunks" / safe_name
    file_output_dir.mkdir(parents=True, exist_ok=True)

    saved_files = []

    for strategy_name, strategy_result in test_results.items():
        if "error" in strategy_result:
            continue

        flattened_chunks = strategy_result.get("flattened_chunks", {})
        if not flattened_chunks or "chunks_preview" not in flattened_chunks:
            continue

        # é‡æ–°è·å–å®Œæ•´çš„chunksï¼ˆpreviewåªæœ‰å‰3ä¸ªï¼‰
        try:
            # æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©å¤„ç†æ–¹æ³•
            if is_legal_document(file_path):
                # ä½¿ç”¨æ³•å¾‹æ–‡æ¡£ä¸“ç”¨åˆ‡åˆ†å™¨
                full_chunks = split_legal_document(
                    file_path,
                    max_tokens=strategy_result["max_tokens"]
                )
            else:
                # é‡æ–°å¤„ç†ä»¥è·å–å®Œæ•´chunks
                splitter = ContractSplitter(
                    max_tokens=strategy_result["max_tokens"],
                    overlap=strategy_result["overlap"],
                    split_by_sentence=True,
                    token_counter="character"
                )
                sections = splitter.split(file_path)
                full_chunks = splitter.flatten(sections)

            # ä¿å­˜chunksåˆ°æ–‡æœ¬æ–‡ä»¶ï¼ˆæ–¹ä¾¿é˜…è¯»ï¼‰
            chunks_txt_file = file_output_dir / f"{strategy_name}_chunks.txt"
            with open(chunks_txt_file, 'w', encoding='utf-8') as f:
                f.write(f"æ–‡ä»¶: {file_path}\n")
                f.write(f"ç­–ç•¥: {strategy_name}\n")
                f.write(f"å‚æ•°: max_tokens={strategy_result['max_tokens']}, overlap={strategy_result['overlap']}\n")
                f.write(f"æ€»chunksæ•°: {len(full_chunks)}\n")
                f.write("=" * 80 + "\n\n")

                for i, chunk in enumerate(full_chunks):
                    f.write(f"ã€Chunk {i+1:03d}ã€‘ (é•¿åº¦: {len(chunk)} å­—ç¬¦)\n")
                    f.write("-" * 40 + "\n")
                    f.write(chunk)
                    f.write("\n" + "=" * 80 + "\n\n")

            # ä¿å­˜chunksåˆ°JSONæ–‡ä»¶ï¼ˆæ–¹ä¾¿ç¨‹åºå¤„ç†ï¼‰
            chunks_json_file = file_output_dir / f"{strategy_name}_chunks.json"
            chunks_data = {
                "file_path": file_path,
                "strategy": strategy_name,
                "parameters": {
                    "max_tokens": strategy_result["max_tokens"],
                    "overlap": strategy_result["overlap"]
                },
                "statistics": {
                    "total_chunks": len(full_chunks),
                    "avg_length": round(sum(len(chunk) for chunk in full_chunks) / len(full_chunks), 2) if full_chunks else 0,
                    "max_length": max(len(chunk) for chunk in full_chunks) if full_chunks else 0,
                    "min_length": min(len(chunk) for chunk in full_chunks) if full_chunks else 0
                },
                "chunks": [
                    {
                        "id": i + 1,
                        "content": chunk,
                        "length": len(chunk)
                    }
                    for i, chunk in enumerate(full_chunks)
                ]
            }

            with open(chunks_json_file, 'w', encoding='utf-8') as f:
                json.dump(chunks_data, f, ensure_ascii=False, indent=2)

            saved_files.extend([str(chunks_txt_file), str(chunks_json_file)])

        except Exception as e:
            print(f"  âš ï¸ æ— æ³•é‡æ–°å¤„ç† {strategy_name}: {e}")

    return saved_files


def save_test_results(all_results: List[Dict[str, Any]], output_dir: str = "output"):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
    print(f"\n{'='*60}")
    print("ä¿å­˜æµ‹è¯•ç»“æœ")
    print(f"{'='*60}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(exist_ok=True)

    # ç”Ÿæˆæ—¶é—´æˆ³
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_file = f"{output_dir}/chunking_test_detailed_{timestamp}.json"
    with open(detailed_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"âœ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: {detailed_file}")

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary_file = f"{output_dir}/chunking_test_summary_{timestamp}.md"
    generate_summary_report(all_results, summary_file)
    print(f"âœ“ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")

    # ä¸ºæ¯ä¸ªæ–‡ä»¶å•ç‹¬ä¿å­˜chunks
    print(f"\nä¿å­˜å•ç‹¬çš„chunksæ–‡ä»¶...")
    all_saved_files = []

    for result in all_results:
        if "error" in result:
            continue

        file_path = result["file_path"]
        test_results = result.get("test_results", {})

        if test_results:
            print(f"  ğŸ“ ä¿å­˜ {Path(file_path).name} çš„chunks...")
            saved_files = save_individual_chunks(file_path, test_results, output_dir)
            all_saved_files.extend(saved_files)
            print(f"    âœ“ ä¿å­˜äº† {len(saved_files)} ä¸ªæ–‡ä»¶")

    print(f"âœ“ æ€»å…±ä¿å­˜äº† {len(all_saved_files)} ä¸ªå•ç‹¬çš„chunksæ–‡ä»¶")

    return detailed_file, summary_file


def generate_summary_report(all_results: List[Dict[str, Any]], output_file: str):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# Chunkingæµ‹è¯•æ±‡æ€»æŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # æ€»ä½“ç»Ÿè®¡
        f.write("## æ€»ä½“ç»Ÿè®¡\n\n")
        total_files = len(all_results)
        successful_files = sum(1 for r in all_results if "error" not in r)
        f.write(f"- æµ‹è¯•æ–‡ä»¶æ€»æ•°: {total_files}\n")
        f.write(f"- æˆåŠŸå¤„ç†æ–‡ä»¶: {successful_files}\n")
        f.write(f"- å¤±è´¥æ–‡ä»¶: {total_files - successful_files}\n\n")

        # æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡
        f.write("## æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡\n\n")
        file_types = {}
        for result in all_results:
            ext = result.get("file_extension", "unknown")
            if ext not in file_types:
                file_types[ext] = {"total": 0, "success": 0}
            file_types[ext]["total"] += 1
            if "error" not in result:
                file_types[ext]["success"] += 1

        for ext, stats in file_types.items():
            f.write(f"- {ext}: {stats['success']}/{stats['total']} æˆåŠŸ\n")
        f.write("\n")

        # è¯¦ç»†ç»“æœ
        f.write("## è¯¦ç»†ç»“æœ\n\n")
        for i, result in enumerate(all_results):
            file_name = Path(result['file_path']).name
            safe_name = "".join(c for c in Path(result['file_path']).stem if c.isalnum() or c in (' ', '-', '_')).rstrip()

            f.write(f"### {i+1}. {file_name}\n\n")
            f.write(f"- æ–‡ä»¶è·¯å¾„: `{result['file_path']}`\n")
            f.write(f"- æ–‡ä»¶å¤§å°: {result.get('file_size', 0)} bytes\n")
            f.write(f"- æ–‡ä»¶ç±»å‹: {result.get('file_extension', 'unknown')}\n")

            if "error" in result:
                f.write(f"- âŒ å¤„ç†å¤±è´¥: {result['error']}\n\n")
                continue

            f.write("- âœ… å¤„ç†æˆåŠŸ\n")
            f.write(f"- ğŸ“ å•ç‹¬chunksæ–‡ä»¶: `output/individual_chunks/{safe_name}/`\n\n")

            # æµ‹è¯•ç­–ç•¥ç»“æœ
            if "test_results" in result:
                f.write("#### Chunkingç­–ç•¥æµ‹è¯•ç»“æœ\n\n")
                for strategy_name, strategy_result in result["test_results"].items():
                    if "error" in strategy_result:
                        f.write(f"- **{strategy_name}**: âŒ {strategy_result['error']}\n")
                    else:
                        sections = strategy_result.get("hierarchical_sections", {}).get("num_sections", 0)
                        chunks = strategy_result.get("flattened_chunks", {}).get("num_chunks", 0)
                        time_taken = strategy_result.get("processing_time", 0)
                        f.write(f"- **{strategy_name}**: {sections} sections â†’ {chunks} chunks ({time_taken}s)\n")
                        f.write(f"  - ğŸ“„ æ–‡æœ¬æ–‡ä»¶: `output/individual_chunks/{safe_name}/{strategy_name}_chunks.txt`\n")
                        f.write(f"  - ğŸ“Š JSONæ–‡ä»¶: `output/individual_chunks/{safe_name}/{strategy_name}_chunks.json`\n")
                f.write("\n")

            # Tokenè®¡æ•°ç»“æœ
            if "token_methods" in result:
                f.write("#### Tokenè®¡æ•°æ–¹æ³•å¯¹æ¯”\n\n")
                for method, method_result in result["token_methods"].items():
                    total_tokens = method_result.get("total_tokens", 0)
                    f.write(f"- **{method}**: {total_tokens} tokens\n")
                f.write("\n")

            # ä¸­æ–‡å¤„ç†ç»“æœ
            if "chinese_processing" in result:
                chinese_result = result["chinese_processing"]
                if chinese_result.get("has_chinese", False):
                    sentences = chinese_result.get("sentences_count", 0)
                    f.write(f"#### ä¸­æ–‡å¤„ç†: âœ… æ£€æµ‹åˆ°ä¸­æ–‡ï¼Œåˆ†å‰²ä¸º {sentences} ä¸ªå¥å­\n\n")
                else:
                    f.write("#### ä¸­æ–‡å¤„ç†: âŒ æœªæ£€æµ‹åˆ°ä¸­æ–‡å†…å®¹\n\n")

        # æ·»åŠ äººå·¥æ ¸å¯¹æŒ‡å—
        f.write("## ğŸ“‹ äººå·¥æ ¸å¯¹æŒ‡å—\n\n")
        f.write("æ¯ä¸ªæˆåŠŸå¤„ç†çš„æ–‡ä»¶éƒ½æœ‰å¯¹åº”çš„chunksæ–‡ä»¶ä¿å­˜åœ¨ `output/individual_chunks/` ç›®å½•ä¸‹ï¼š\n\n")
        f.write("### æ–‡ä»¶ç»“æ„\n")
        f.write("```\n")
        f.write("output/individual_chunks/\n")
        f.write("â”œâ”€â”€ æ–‡ä»¶å1/\n")
        f.write("â”‚   â”œâ”€â”€ small_chunks_chunks.txt    # å°å—ç­–ç•¥çš„æ–‡æœ¬æ–‡ä»¶\n")
        f.write("â”‚   â”œâ”€â”€ small_chunks_chunks.json   # å°å—ç­–ç•¥çš„JSONæ–‡ä»¶\n")
        f.write("â”‚   â”œâ”€â”€ medium_chunks_chunks.txt   # ä¸­å—ç­–ç•¥çš„æ–‡æœ¬æ–‡ä»¶\n")
        f.write("â”‚   â”œâ”€â”€ medium_chunks_chunks.json  # ä¸­å—ç­–ç•¥çš„JSONæ–‡ä»¶\n")
        f.write("â”‚   â”œâ”€â”€ large_chunks_chunks.txt    # å¤§å—ç­–ç•¥çš„æ–‡æœ¬æ–‡ä»¶\n")
        f.write("â”‚   â””â”€â”€ large_chunks_chunks.json   # å¤§å—ç­–ç•¥çš„JSONæ–‡ä»¶\n")
        f.write("â””â”€â”€ æ–‡ä»¶å2/\n")
        f.write("    â””â”€â”€ ...\n")
        f.write("```\n\n")
        f.write("### æ ¸å¯¹è¦ç‚¹\n\n")
        f.write("1. **å†…å®¹å®Œæ•´æ€§**: æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹ä¸¢å¤±æˆ–é‡å¤\n")
        f.write("2. **è¡¨æ ¼ç»“æ„**: éªŒè¯è¡¨æ ¼å†…å®¹æ˜¯å¦æ­£ç¡®æå–å’Œæ ¼å¼åŒ–\n")
        f.write("3. **ä¸­æ–‡å¤„ç†**: ç¡®è®¤ä¸­æ–‡æ–‡æœ¬åˆ†å‰²æ˜¯å¦åˆç†\n")
        f.write("4. **å±‚æ¬¡ç»“æ„**: æ£€æŸ¥ç« èŠ‚æ ‡é¢˜å’Œå†…å®¹çš„å¯¹åº”å…³ç³»\n")
        f.write("5. **chunkè¾¹ç•Œ**: éªŒè¯chunkåˆ†å‰²ç‚¹æ˜¯å¦åˆç†ï¼ˆé¿å…å¥å­æˆªæ–­ï¼‰\n\n")
        f.write("### æ¨èæ ¸å¯¹æµç¨‹\n\n")
        f.write("1. å…ˆæŸ¥çœ‹ `.txt` æ–‡ä»¶è¿›è¡Œå¿«é€Ÿæµè§ˆ\n")
        f.write("2. å¯¹æ¯”ä¸åŒç­–ç•¥çš„åˆ†å‰²æ•ˆæœ\n")
        f.write("3. é‡ç‚¹æ£€æŸ¥è¡¨æ ¼å¯†é›†çš„æ–‡æ¡£\n")
        f.write("4. éªŒè¯å…³é”®ä¿¡æ¯æ˜¯å¦å®Œæ•´ä¿ç•™\n")


def run_comprehensive_tests():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ç»¼åˆChunkingæµ‹è¯•")
    print("=" * 80)

    # æŸ¥æ‰¾æ–‡æ¡£æ–‡ä»¶
    document_files = find_document_files()

    if not document_files:
        print("âŒ åœ¨outputç›®å½•ä¸‹æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶ (.doc, .docx, .pdf)")
        print("è¯·å°†æµ‹è¯•æ–‡æ¡£æ”¾å…¥outputç›®å½•")
        return

    print(f"âœ… æ‰¾åˆ° {len(document_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶:")
    for file in document_files:
        print(f"  - {file}")

    all_results = []

    # å¯¹æ¯ä¸ªæ–‡ä»¶è¿›è¡Œæµ‹è¯•
    for i, file_path in enumerate(document_files):
        print(f"\nğŸ”„ å¤„ç†æ–‡ä»¶ {i+1}/{len(document_files)}: {Path(file_path).name}")

        # åŸºæœ¬chunkingæµ‹è¯•
        chunking_result = test_file_chunking(file_path)

        # Tokenè®¡æ•°å¯¹æ¯”æµ‹è¯•
        token_result = test_token_counting_comparison(file_path)

        # ä¸­æ–‡å¤„ç†æµ‹è¯•
        chinese_result = test_chinese_text_processing(file_path)

        # åˆå¹¶ç»“æœ
        combined_result = {
            **chunking_result,
            **token_result,
            **chinese_result
        }

        all_results.append(combined_result)

    # ä¿å­˜ç»“æœ
    detailed_file, summary_file = save_test_results(all_results)

    # æ‰“å°æ€»ç»“
    print(f"\n{'='*80}")
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    print(f"{'='*80}")
    print(f"ğŸ“Š è¯¦ç»†ç»“æœ: {detailed_file}")
    print(f"ğŸ“‹ æ±‡æ€»æŠ¥å‘Š: {summary_file}")
    print(f"ğŸ“ æ€»å…±æµ‹è¯•äº† {len(document_files)} ä¸ªæ–‡ä»¶")

    successful_count = sum(1 for r in all_results if "error" not in r)
    print(f"âœ… æˆåŠŸ: {successful_count} ä¸ªæ–‡ä»¶")
    print(f"âŒ å¤±è´¥: {len(document_files) - successful_count} ä¸ªæ–‡ä»¶")

    return all_results


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="ç»¼åˆChunkingæµ‹è¯•å·¥å…·")
    parser.add_argument("--file", "-f", help="æµ‹è¯•å•ä¸ªæ–‡ä»¶")
    parser.add_argument("--output-dir", "-o", default="output", help="è¾“å‡ºç›®å½•")

    args = parser.parse_args()

    if args.file:
        # æµ‹è¯•å•ä¸ªæ–‡ä»¶
        if not os.path.exists(args.file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
            return 1

        print(f"ğŸ”„ æµ‹è¯•å•ä¸ªæ–‡ä»¶: {args.file}")
        result = test_file_chunking(args.file)

        # ä¿å­˜å•ä¸ªæ–‡ä»¶ç»“æœ
        output_file = f"{args.output_dir}/single_file_test_{Path(args.file).stem}.json"
        Path(args.output_dir).mkdir(exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"âœ… ç»“æœå·²ä¿å­˜: {output_file}")

        # ä¿å­˜å•ç‹¬çš„chunksæ–‡ä»¶
        if "error" not in result and "test_results" in result:
            print(f"ğŸ“ ä¿å­˜å•ç‹¬çš„chunksæ–‡ä»¶...")
            saved_files = save_individual_chunks(args.file, result["test_results"], args.output_dir)
            print(f"âœ… ä¿å­˜äº† {len(saved_files)} ä¸ªchunksæ–‡ä»¶:")
            for file_path in saved_files:
                print(f"  ğŸ“„ {file_path}")
        else:
            print(f"âš ï¸ æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œæ— æ³•ä¿å­˜chunks")
    else:
        # è¿è¡Œç»¼åˆæµ‹è¯•
        run_comprehensive_tests()

    return 0


if __name__ == "__main__":
    import datetime
    exit(main())


