#!/usr/bin/env python3
"""
ç›´æ¥æµ‹è¯•ä¸¥æ ¼chunkæ§åˆ¶åŠŸèƒ½
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from contract_splitter import DocxSplitter


def test_direct_strict_control():
    """ç›´æ¥æµ‹è¯•ä¸¥æ ¼æ§åˆ¶åŠŸèƒ½"""
    print("ğŸ”§ ç›´æ¥æµ‹è¯•ä¸¥æ ¼chunkæ§åˆ¶åŠŸèƒ½")
    print("=" * 80)
    
    # åˆ›å»ºä¸€ä¸ªè¶…é•¿çš„chunk
    long_chunk = """è¿™æ˜¯ä¸€ä¸ªè¶…é•¿çš„æ–‡æœ¬å—ã€‚""" * 100  # é‡å¤100æ¬¡
    
    print(f"åŸå§‹chunké•¿åº¦: {len(long_chunk)} å­—ç¬¦")
    
    # åˆ›å»ºsplitter
    splitter = DocxSplitter(
        max_tokens=200,  # è®¾ç½®å¾ˆå°çš„é™åˆ¶
        overlap=50,
        strict_max_tokens=True
    )
    
    # ç›´æ¥æµ‹è¯•_apply_strict_max_tokensæ–¹æ³•
    print("\nç›´æ¥è°ƒç”¨_apply_strict_max_tokensæ–¹æ³•:")
    result_chunks = splitter._apply_strict_max_tokens([long_chunk])
    
    print(f"åˆ†å‰²åchunksæ•°é‡: {len(result_chunks)}")
    for i, chunk in enumerate(result_chunks):
        print(f"  Chunk {i+1}: {len(chunk)} å­—ç¬¦")
        if len(chunk) > 200:
            print(f"    âš ï¸ ä»ç„¶è¶…è¿‡é™åˆ¶!")
        else:
            print(f"    âœ… åœ¨é™åˆ¶å†…")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªchunkçš„å†…å®¹
    print(f"\nå‰3ä¸ªchunkçš„å†…å®¹:")
    for i, chunk in enumerate(result_chunks[:3]):
        print(f"Chunk {i+1}: {chunk[:50]}...")


def test_sentence_splitting_direct():
    """ç›´æ¥æµ‹è¯•å¥å­åˆ†å‰²åŠŸèƒ½"""
    print("\nâœ‚ï¸ ç›´æ¥æµ‹è¯•å¥å­åˆ†å‰²åŠŸèƒ½")
    print("=" * 80)
    
    # åˆ›å»ºåŒ…å«å¤šä¸ªå¥å­çš„é•¿æ–‡æœ¬
    long_text = """
ç¬¬ä¸€ä¸ªå¥å­åŒ…å«ä¸€äº›å†…å®¹ã€‚ç¬¬äºŒä¸ªå¥å­ä¹ŸåŒ…å«å†…å®¹ï¼ç¬¬ä¸‰ä¸ªå¥å­åŒ…å«æ›´å¤šå†…å®¹ï¼Ÿç¬¬å››ä¸ªå¥å­ç»§ç»­ï¼›ç¬¬äº”ä¸ªå¥å­ç»“æŸã€‚
è¿™æ˜¯å¦ä¸€ä¸ªå¾ˆé•¿çš„å¥å­ï¼Œå®ƒåŒ…å«äº†å¤§é‡çš„æ–‡å­—å†…å®¹ï¼Œç›®çš„æ˜¯æµ‹è¯•å½“å•ä¸ªå¥å­è¶…è¿‡é™åˆ¶æ—¶ç³»ç»Ÿçš„å¤„ç†èƒ½åŠ›å’Œåˆ†å‰²ç­–ç•¥ã€‚
ç¬¬å…­ä¸ªå¥å­æ¯”è¾ƒçŸ­ã€‚ç¬¬ä¸ƒä¸ªå¥å­ä¹Ÿå¾ˆçŸ­ã€‚ç¬¬å…«ä¸ªå¥å­ç¨å¾®é•¿ä¸€äº›ä½†ä»ç„¶åœ¨åˆç†èŒƒå›´å†…ã€‚
"""
    
    print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(long_text)} å­—ç¬¦")
    
    splitter = DocxSplitter(
        max_tokens=100,  # å¾ˆå°çš„é™åˆ¶
        overlap=20,
        strict_max_tokens=True
    )
    
    # ç›´æ¥æµ‹è¯•_split_oversized_chunkæ–¹æ³•
    print("\nç›´æ¥è°ƒç”¨_split_oversized_chunkæ–¹æ³•:")
    result_chunks = splitter._split_oversized_chunk(long_text)
    
    print(f"åˆ†å‰²åchunksæ•°é‡: {len(result_chunks)}")
    for i, chunk in enumerate(result_chunks):
        print(f"  Chunk {i+1}: {len(chunk)} å­—ç¬¦")
        print(f"    å†…å®¹: {chunk.strip()[:80]}...")
        print()


def test_overlap_direct():
    """ç›´æ¥æµ‹è¯•overlapåŠŸèƒ½"""
    print("ğŸ”„ ç›´æ¥æµ‹è¯•overlapåŠŸèƒ½")
    print("=" * 80)
    
    test_text = "è¿™æ˜¯ç¬¬ä¸€å¥ã€‚è¿™æ˜¯ç¬¬äºŒå¥ã€‚è¿™æ˜¯ç¬¬ä¸‰å¥ã€‚è¿™æ˜¯ç¬¬å››å¥ã€‚è¿™æ˜¯ç¬¬äº”å¥ã€‚è¿™æ˜¯ç¬¬å…­å¥ã€‚"
    
    splitter = DocxSplitter(
        max_tokens=30,  # å¾ˆå°çš„é™åˆ¶
        overlap=10,
        strict_max_tokens=True
    )
    
    # ç›´æ¥æµ‹è¯•overlapåŠŸèƒ½
    result_chunks = splitter._split_oversized_chunk(test_text)
    
    print(f"åŸå§‹æ–‡æœ¬: {test_text}")
    print(f"åˆ†å‰²åchunksæ•°é‡: {len(result_chunks)}")
    
    for i, chunk in enumerate(result_chunks):
        print(f"Chunk {i+1}: {chunk}")
    
    # æ£€æŸ¥overlap
    print("\næ£€æŸ¥overlap:")
    for i in range(len(result_chunks) - 1):
        current = result_chunks[i]
        next_chunk = result_chunks[i + 1]
        
        # ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰é‡å å†…å®¹
        current_words = current.split()[-3:]  # å–æœ€å3ä¸ªè¯
        next_words = next_chunk.split()[:5]   # å–å‰5ä¸ªè¯
        
        overlap_found = any(word in next_words for word in current_words if len(word) > 1)
        print(f"  Chunk {i+1} -> Chunk {i+2}: {'âœ… æœ‰overlap' if overlap_found else 'âŒ æ— overlap'}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª ä¸¥æ ¼chunkæ§åˆ¶åŠŸèƒ½ç›´æ¥æµ‹è¯•")
    print("=" * 80)
    
    test_direct_strict_control()
    test_sentence_splitting_direct()
    test_overlap_direct()
    
    print("\n" + "=" * 80)
    print("ğŸ¯ ç›´æ¥æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()
