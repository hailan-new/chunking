#!/usr/bin/env python3
"""
æµ‹è¯•å»é‡åŠŸèƒ½
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from contract_splitter import ContractSplitter


def advanced_deduplication(chunks):
    """
    é«˜çº§å»é‡åŠŸèƒ½ï¼šæ£€æµ‹å¹¶ç§»é™¤é‡å¤å’Œé«˜åº¦ç›¸ä¼¼çš„chunks
    
    Args:
        chunks: åŸå§‹chunksåˆ—è¡¨
        
    Returns:
        å»é‡åçš„chunksåˆ—è¡¨
    """
    if not chunks:
        return chunks
    
    unique_chunks = []
    seen_fingerprints = set()
    
    for i, chunk in enumerate(chunks):
        # åˆ›å»ºå†…å®¹æŒ‡çº¹
        fingerprint = create_content_fingerprint(chunk)
        
        # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰chunksé‡å¤
        is_duplicate = False
        for seen_fp in seen_fingerprints:
            if chunks_are_similar(fingerprint, seen_fp, threshold=0.7):
                print(f"ğŸ” å‘ç°é‡å¤chunk {i+1}: ä¸ä¹‹å‰çš„chunkç›¸ä¼¼åº¦è¿‡é«˜")
                is_duplicate = True
                break
        
        if not is_duplicate:
            seen_fingerprints.add(fingerprint)
            unique_chunks.append(chunk)
        
    return unique_chunks


def create_content_fingerprint(text):
    """
    åˆ›å»ºå†…å®¹æŒ‡çº¹ï¼Œç”¨äºç›¸ä¼¼æ€§æ£€æµ‹
    
    Args:
        text: æ–‡æœ¬å†…å®¹
        
    Returns:
        å†…å®¹æŒ‡çº¹å­—ç¬¦ä¸²
    """
    # ç§»é™¤æ ¼å¼æ ‡è®°å’Œç©ºç™½å­—ç¬¦
    import re
    
    # ç§»é™¤chunkæ ‡é¢˜å’Œåˆ†éš”ç¬¦
    clean_text = re.sub(r'ã€Chunk \d+ã€‘.*?\n', '', text)
    clean_text = re.sub(r'={50,}', '', clean_text)
    clean_text = re.sub(r'-{20,}', '', clean_text)
    clean_text = re.sub(r'\(é•¿åº¦: \d+ å­—ç¬¦\)', '', clean_text)
    
    # ç§»é™¤å¤šä½™ç©ºç™½
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    # å–å‰500å­—ç¬¦ä½œä¸ºæŒ‡çº¹
    return clean_text[:500]


def chunks_are_similar(text1, text2, threshold=0.7):
    """
    æ£€æŸ¥ä¸¤ä¸ªæ–‡æœ¬æ˜¯å¦ç›¸ä¼¼
    
    Args:
        text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
        text2: ç¬¬äºŒä¸ªæ–‡æœ¬
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
    Returns:
        True if similar
    """
    if not text1 or not text2:
        return False
    
    # è®¡ç®—å­—ç¬¦çº§åˆ«çš„ç›¸ä¼¼åº¦
    set1 = set(text1.lower())
    set2 = set(text2.lower())
    
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    
    if union == 0:
        return False
    
    similarity = intersection / union
    return similarity >= threshold


def test_deduplication():
    """æµ‹è¯•å»é‡åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•å»é‡åŠŸèƒ½")
    print("=" * 60)
    
    # æµ‹è¯•æ–‡ä»¶
    test_file = "output/ã€ç«‹é¡¹ç”³è¯·ã€‘é¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„å¹¿å·å†œå•†è¡Œçš„ç«‹é¡¹ç”³è¯·.doc"
    
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return False
    
    try:
        # åˆ›å»ºåˆ†å‰²å™¨
        splitter = ContractSplitter(
            max_tokens=1000,
            overlap=100,
            split_by_sentence=True,
            token_counter="character"
        )
        
        # åˆ†å‰²æ–‡æ¡£
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {test_file}")
        sections = splitter.split(test_file)
        original_chunks = splitter.flatten(sections)
        
        print(f"ğŸ“Š åŸå§‹chunksæ•°é‡: {len(original_chunks)}")
        
        # åº”ç”¨é«˜çº§å»é‡
        deduplicated_chunks = advanced_deduplication(original_chunks)
        
        print(f"ğŸ“Š å»é‡åchunksæ•°é‡: {len(deduplicated_chunks)}")
        print(f"ğŸ“Š å»é™¤é‡å¤chunks: {len(original_chunks) - len(deduplicated_chunks)} ä¸ª")
        
        # æ£€æŸ¥ç‰¹å®šé‡å¤å†…å®¹
        duplicate_patterns = [
            "ä¸€ã€é¡¹ç›®åç§°ï¼šé¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„-å¹¿å·å†œå•†è¡Œ",
            "äºŒã€é¡¹ç›®èƒŒæ™¯ï¼š",
            "å››ã€ä»£é”€æœºæ„ä»‹ç»"
        ]
        
        print("\nğŸ” æ£€æŸ¥ç‰¹å®šé‡å¤æ¨¡å¼:")
        for pattern in duplicate_patterns:
            original_count = sum(1 for chunk in original_chunks if pattern in chunk)
            deduplicated_count = sum(1 for chunk in deduplicated_chunks if pattern in chunk)
            
            print(f"  '{pattern[:20]}...': {original_count} -> {deduplicated_count}")
        
        # ä¿å­˜å»é‡åçš„ç»“æœ
        output_file = "output/deduplicated_chunks.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            for i, chunk in enumerate(deduplicated_chunks, 1):
                f.write(f"ã€Chunk {i:03d}ã€‘ (é•¿åº¦: {len(chunk)} å­—ç¬¦)\n")
                f.write("-" * 40 + "\n")
                f.write(chunk + "\n")
                f.write("=" * 80 + "\n\n")
        
        print(f"\nâœ… å»é‡åçš„ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å»é‡åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    
    success = test_deduplication()
    
    print("\n" + "=" * 80)
    if success:
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼è¯·æŸ¥çœ‹å»é‡æ•ˆæœ")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼")
    
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
