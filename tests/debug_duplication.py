#!/usr/bin/env python3
"""
è¯Šæ–­é‡å¤å†…å®¹äº§ç”Ÿçš„åŸå› 
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from contract_splitter import ContractSplitter


def debug_duplication_source():
    """è°ƒè¯•é‡å¤å†…å®¹çš„äº§ç”Ÿæºå¤´"""
    print("ğŸ” è°ƒè¯•é‡å¤å†…å®¹äº§ç”Ÿæºå¤´")
    print("=" * 80)
    
    test_file = "output/ã€ç«‹é¡¹ç”³è¯·ã€‘é¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„å¹¿å·å†œå•†è¡Œçš„ç«‹é¡¹ç”³è¯·.doc"
    
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    # åˆ›å»ºåˆ†å‰²å™¨
    splitter = ContractSplitter(
        max_tokens=1000,
        overlap=100,
        split_by_sentence=True,
        token_counter="character"
    )
    
    print("ğŸ“„ æ­¥éª¤1: åˆ†å‰²æ–‡æ¡£ä¸ºsections")
    sections = splitter.split(test_file)
    print(f"   å¾—åˆ° {len(sections)} ä¸ªsections")
    
    # åˆ†ææ¯ä¸ªsectionçš„å†…å®¹
    for i, section in enumerate(sections):
        print(f"\nğŸ“‹ Section {i+1}:")
        print(f"   æ ‡é¢˜: {section.get('heading', 'N/A')}")
        print(f"   å†…å®¹é•¿åº¦: {len(section.get('content', ''))}")
        print(f"   å­ç« èŠ‚æ•°: {len(section.get('subsections', []))}")
        
        # æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«é‡å¤çš„å…³é”®è¯
        content = section.get('content', '')
        key_phrases = [
            "ä¸€ã€é¡¹ç›®åç§°ï¼šé¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„-å¹¿å·å†œå•†è¡Œ",
            "äºŒã€é¡¹ç›®èƒŒæ™¯ï¼š",
            "å››ã€ä»£é”€æœºæ„ä»‹ç»"
        ]
        
        for phrase in key_phrases:
            count = content.count(phrase)
            if count > 0:
                print(f"   ğŸ” åŒ…å« '{phrase[:30]}...': {count} æ¬¡")
        
        # æ˜¾ç¤ºå†…å®¹çš„å‰200å­—ç¬¦
        if content:
            preview = content[:200].replace('\n', ' ')
            print(f"   å†…å®¹é¢„è§ˆ: {preview}...")
    
    print("\nğŸ“„ æ­¥éª¤2: å±•å¹³sectionsä¸ºchunks")
    chunks = splitter.flatten(sections)
    print(f"   å¾—åˆ° {len(chunks)} ä¸ªchunks")
    
    # åˆ†æchunksä¸­çš„é‡å¤
    print("\nğŸ” åˆ†æchunksä¸­çš„é‡å¤å†…å®¹:")
    key_phrases = [
        "ä¸€ã€é¡¹ç›®åç§°ï¼šé¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„-å¹¿å·å†œå•†è¡Œ",
        "äºŒã€é¡¹ç›®èƒŒæ™¯ï¼š",
        "å››ã€ä»£é”€æœºæ„ä»‹ç»"
    ]
    
    for phrase in key_phrases:
        matching_chunks = []
        for i, chunk in enumerate(chunks):
            if phrase in chunk:
                matching_chunks.append(i + 1)
        
        if len(matching_chunks) > 1:
            print(f"   âš ï¸  '{phrase[:30]}...' å‡ºç°åœ¨chunks: {matching_chunks}")
    
    # è¯¦ç»†åˆ†æé‡å¤chunksçš„å†…å®¹æ¥æº
    print("\nğŸ” è¯¦ç»†åˆ†æé‡å¤chunks:")
    target_phrase = "ä¸€ã€é¡¹ç›®åç§°ï¼šé¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„-å¹¿å·å†œå•†è¡Œ"
    
    for i, chunk in enumerate(chunks):
        if target_phrase in chunk:
            print(f"\n   ğŸ“‹ Chunk {i+1} (åŒ…å«ç›®æ ‡çŸ­è¯­):")
            print(f"      é•¿åº¦: {len(chunk)} å­—ç¬¦")
            
            # æŸ¥æ‰¾è¿™ä¸ªçŸ­è¯­åœ¨chunkä¸­çš„ä½ç½®
            start_pos = chunk.find(target_phrase)
            end_pos = start_pos + 100  # æ˜¾ç¤ºçŸ­è¯­å100å­—ç¬¦
            
            context = chunk[max(0, start_pos-50):end_pos]
            print(f"      ä¸Šä¸‹æ–‡: ...{context}...")
            
            # æ£€æŸ¥chunkçš„æ¥æºï¼ˆé€šè¿‡æ ‡é¢˜åˆ¤æ–­ï¼‰
            lines = chunk.split('\n')
            for line in lines[:3]:  # æ£€æŸ¥å‰3è¡Œ
                if '>' in line or 'Part' in line:
                    print(f"      æ¥æºæ ‡è¯†: {line}")
                    break


def analyze_table_extraction():
    """åˆ†æè¡¨æ ¼æå–è¿‡ç¨‹"""
    print("\n" + "=" * 80)
    print("ğŸ” åˆ†æè¡¨æ ¼æå–è¿‡ç¨‹")
    print("=" * 80)
    
    test_file = "output/ã€ç«‹é¡¹ç”³è¯·ã€‘é¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„å¹¿å·å†œå•†è¡Œçš„ç«‹é¡¹ç”³è¯·.doc"
    
    # ç›´æ¥ä½¿ç”¨DocxSplitteræ¥æŸ¥çœ‹è¡¨æ ¼æå–
    from contract_splitter.docx_splitter import DocxSplitter
    
    docx_splitter = DocxSplitter(max_tokens=1000, overlap=100)
    
    print("ğŸ“„ ç›´æ¥ä½¿ç”¨DocxSplitteråˆ†æ...")
    
    try:
        sections = docx_splitter.split(test_file)
        
        print(f"DocxSplitterå¾—åˆ° {len(sections)} ä¸ªsections")
        
        for i, section in enumerate(sections):
            content = section.get('content', '')
            if 'ä¸€ã€é¡¹ç›®åç§°' in content:
                print(f"\nğŸ“‹ Section {i+1} åŒ…å«ç›®æ ‡å†…å®¹:")
                print(f"   æ ‡é¢˜: {section.get('heading', 'N/A')}")
                print(f"   å†…å®¹é•¿åº¦: {len(content)}")
                
                # è®¡ç®—ç›®æ ‡çŸ­è¯­å‡ºç°æ¬¡æ•°
                target_count = content.count("ä¸€ã€é¡¹ç›®åç§°ï¼šé¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„-å¹¿å·å†œå•†è¡Œ")
                print(f"   ç›®æ ‡çŸ­è¯­å‡ºç°æ¬¡æ•°: {target_count}")
                
                if target_count > 1:
                    print("   âš ï¸  åœ¨å•ä¸ªsectionä¸­å°±æœ‰é‡å¤ï¼")
                    
                    # æ‰¾åˆ°æ‰€æœ‰å‡ºç°ä½ç½®
                    positions = []
                    start = 0
                    while True:
                        pos = content.find("ä¸€ã€é¡¹ç›®åç§°ï¼šé¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„-å¹¿å·å†œå•†è¡Œ", start)
                        if pos == -1:
                            break
                        positions.append(pos)
                        start = pos + 1
                    
                    print(f"   å‡ºç°ä½ç½®: {positions}")
                    
                    # æ˜¾ç¤ºæ¯ä¸ªä½ç½®çš„ä¸Šä¸‹æ–‡
                    for j, pos in enumerate(positions):
                        context_start = max(0, pos - 50)
                        context_end = min(len(content), pos + 150)
                        context = content[context_start:context_end]
                        print(f"   ä½ç½®{j+1}ä¸Šä¸‹æ–‡: ...{context}...")
    
    except Exception as e:
        print(f"âŒ DocxSplitteråˆ†æå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é‡å¤å†…å®¹äº§ç”Ÿæºå¤´è°ƒè¯•")
    print("=" * 80)
    
    debug_duplication_source()
    analyze_table_extraction()
    
    print("\n" + "=" * 80)
    print("ğŸ¯ è°ƒè¯•å®Œæˆ")


if __name__ == "__main__":
    main()
