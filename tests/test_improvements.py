#!/usr/bin/env python3
"""
æµ‹è¯•æ–°å¢çš„æ”¹è¿›åŠŸèƒ½
1. æ–‡æœ¬æ¸…ç†ï¼ˆä¿ç•™æ¢è¡Œï¼‰
2. ä¸¥æ ¼chunkå¤§å°æ§åˆ¶
3. ä¸“ä¸šé¢†åŸŸhelperå‡½æ•°
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from contract_splitter import DocxSplitter
from contract_splitter.domain_helpers import (
    split_legal_document,
    split_contract,
    split_regulation,
    LegalClauseSplitter,
    DomainContractSplitter,
    RegulationSplitter
)


def test_text_cleaning():
    """æµ‹è¯•æ–‡æœ¬æ¸…ç†åŠŸèƒ½"""
    print("ğŸ§¹ æµ‹è¯•æ–‡æœ¬æ¸…ç†åŠŸèƒ½")
    print("=" * 80)
    
    from contract_splitter.utils import clean_text
    
    test_texts = [
        "è¿™æ˜¯  ä¸€ä¸ª  æœ‰  å¤šä½™  ç©ºæ ¼  çš„  æ–‡æœ¬",
        "ä¸­æ–‡ å’Œ English æ··åˆ çš„ text",
        "ä¿ç•™\næ¢è¡Œ\nç¬¦å·\nçš„æ–‡æœ¬",
        "å»é™¤  å¤šä½™  ç©ºæ ¼\nä½†æ˜¯\nä¿ç•™\næ¢è¡Œ",
        "æ ‡ç‚¹  ç¬¦å·  ï¼Œ  ä¹Ÿè¦  å¤„ç†  ã€‚",
        "ï¼ˆ  æ‹¬å·  ï¼‰  å’Œ  ã€  æ–¹æ‹¬å·  ã€‘"
    ]
    
    for i, text in enumerate(test_texts):
        cleaned = clean_text(text)
        print(f"æµ‹è¯• {i+1}:")
        print(f"  åŸæ–‡: {repr(text)}")
        print(f"  æ¸…ç†: {repr(cleaned)}")
        print()


def test_strict_max_tokens():
    """æµ‹è¯•ä¸¥æ ¼chunkå¤§å°æ§åˆ¶"""
    print("ğŸ“ æµ‹è¯•ä¸¥æ ¼chunkå¤§å°æ§åˆ¶")
    print("=" * 80)
    
    test_file = "output/ã€ç«‹é¡¹ç”³è¯·ã€‘é¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„å¹¿å·å†œå•†è¡Œçš„ç«‹é¡¹ç”³è¯·.doc"
    
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    # æµ‹è¯•ä¸ä¸¥æ ¼æ§åˆ¶ï¼ˆé»˜è®¤ï¼‰
    print("ğŸ“‹ ä¸ä¸¥æ ¼æ§åˆ¶chunkå¤§å°:")
    splitter_loose = DocxSplitter(
        max_tokens=1000,  # è®¾ç½®è¾ƒå°çš„é™åˆ¶æ¥æµ‹è¯•
        strict_max_tokens=False
    )
    
    sections = splitter_loose.split(test_file)
    chunks_loose = splitter_loose.flatten(sections)
    
    print(f"  æ€»chunks: {len(chunks_loose)}")
    oversized_loose = [i for i, chunk in enumerate(chunks_loose)
                      if len(chunk) > 1000]
    print(f"  è¶…è¿‡1000å­—ç¬¦çš„chunks: {len(oversized_loose)}")
    if oversized_loose:
        for i in oversized_loose[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            size = len(chunks_loose[i])
            print(f"    Chunk {i+1}: {size} å­—ç¬¦")
    
    # æµ‹è¯•ä¸¥æ ¼æ§åˆ¶
    print("\nğŸ“‹ ä¸¥æ ¼æ§åˆ¶chunkå¤§å°:")
    splitter_strict = DocxSplitter(
        max_tokens=1000,
        overlap=100,
        strict_max_tokens=True
    )
    
    sections = splitter_strict.split(test_file)
    chunks_strict = splitter_strict.flatten(sections)
    
    print(f"  æ€»chunks: {len(chunks_strict)}")
    oversized_strict = [i for i, chunk in enumerate(chunks_strict)
                       if len(chunk) > 1000]
    print(f"  è¶…è¿‡1000å­—ç¬¦çš„chunks: {len(oversized_strict)}")
    if oversized_strict:
        for i in oversized_strict[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            size = len(chunks_strict[i])
            print(f"    Chunk {i+1}: {size} å­—ç¬¦")
    
    # æ˜¾ç¤ºchunkå¤§å°åˆ†å¸ƒ
    print(f"\nğŸ“Š Chunkå¤§å°åˆ†å¸ƒ:")
    print(f"  ä¸ä¸¥æ ¼æ§åˆ¶: å¹³å‡{sum(len(c) for c in chunks_loose)/len(chunks_loose):.0f}å­—ç¬¦")
    print(f"  ä¸¥æ ¼æ§åˆ¶: å¹³å‡{sum(len(c) for c in chunks_strict)/len(chunks_strict):.0f}å­—ç¬¦")


def test_domain_helpers():
    """æµ‹è¯•ä¸“ä¸šé¢†åŸŸhelperå‡½æ•°"""
    print("ğŸ›ï¸ æµ‹è¯•ä¸“ä¸šé¢†åŸŸhelperå‡½æ•°")
    print("=" * 80)
    
    test_file = "output/ã€ç«‹é¡¹ç”³è¯·ã€‘é¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„å¹¿å·å†œå•†è¡Œçš„ç«‹é¡¹ç”³è¯·.doc"
    
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    # æµ‹è¯•æ³•å¾‹æ¡æ¬¾åˆ‡åˆ†å™¨
    print("âš–ï¸ æ³•å¾‹æ¡æ¬¾åˆ‡åˆ†å™¨:")
    try:
        legal_chunks = split_legal_document(test_file, max_tokens=1500)
        print(f"  æ³•å¾‹æ–‡æ¡£åˆ‡åˆ†: {len(legal_chunks)} ä¸ªchunks")
        print(f"  å¹³å‡é•¿åº¦: {sum(len(c) for c in legal_chunks)/len(legal_chunks):.0f} å­—ç¬¦")
        print(f"  æœ€å¤§é•¿åº¦: {max(len(c) for c in legal_chunks)} å­—ç¬¦")
    except Exception as e:
        print(f"  âŒ æ³•å¾‹åˆ‡åˆ†å¤±è´¥: {e}")
    
    # æµ‹è¯•åˆåŒåˆ‡åˆ†å™¨
    print("\nğŸ“„ åˆåŒåˆ‡åˆ†å™¨:")
    contract_types = ["general", "service", "purchase"]
    
    for contract_type in contract_types:
        try:
            contract_chunks = split_contract(test_file, contract_type=contract_type)
            print(f"  {contract_type}åˆåŒ: {len(contract_chunks)} ä¸ªchunks")
        except Exception as e:
            print(f"  âŒ {contract_type}åˆåŒåˆ‡åˆ†å¤±è´¥: {e}")
    
    # æµ‹è¯•è§„ç« åˆ¶åº¦åˆ‡åˆ†å™¨
    print("\nğŸ“‹ è§„ç« åˆ¶åº¦åˆ‡åˆ†å™¨:")
    regulation_types = ["general", "hr", "finance"]
    
    for regulation_type in regulation_types:
        try:
            regulation_chunks = split_regulation(test_file, regulation_type=regulation_type)
            print(f"  {regulation_type}è§„ç« : {len(regulation_chunks)} ä¸ªchunks")
        except Exception as e:
            print(f"  âŒ {regulation_type}è§„ç« åˆ‡åˆ†å¤±è´¥: {e}")


def test_domain_splitter_classes():
    """æµ‹è¯•ä¸“ä¸šé¢†åŸŸåˆ‡åˆ†å™¨ç±»"""
    print("ğŸ”§ æµ‹è¯•ä¸“ä¸šé¢†åŸŸåˆ‡åˆ†å™¨ç±»")
    print("=" * 80)
    
    test_file = "output/ã€ç«‹é¡¹ç”³è¯·ã€‘é¦–åˆ›è¯åˆ¸æ–°å¢ä»£é”€æœºæ„å¹¿å·å†œå•†è¡Œçš„ç«‹é¡¹ç”³è¯·.doc"
    
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    # æµ‹è¯•æ³•å¾‹æ¡æ¬¾åˆ‡åˆ†å™¨ç±»
    print("âš–ï¸ LegalClauseSplitter:")
    try:
        legal_splitter = LegalClauseSplitter(
            max_tokens=1200,
            strict_max_tokens=True
        )
        legal_chunks = legal_splitter.split_legal_document(test_file)
        print(f"  åˆ‡åˆ†ç»“æœ: {len(legal_chunks)} ä¸ªchunks")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶…è¿‡é™åˆ¶çš„chunks
        oversized = [i for i, chunk in enumerate(legal_chunks)
                    if len(chunk) > 1200]
        print(f"  è¶…è¿‡é™åˆ¶çš„chunks: {len(oversized)}")
        
    except Exception as e:
        print(f"  âŒ å¤±è´¥: {e}")
    
    # æµ‹è¯•åˆåŒåˆ‡åˆ†å™¨ç±»
    print("\nğŸ“„ DomainContractSplitter:")
    try:
        contract_splitter = DomainContractSplitter(
            contract_type="service",
            max_tokens=1500,
            strict_max_tokens=True
        )
        contract_chunks = contract_splitter.split_contract(test_file)
        print(f"  æœåŠ¡åˆåŒåˆ‡åˆ†: {len(contract_chunks)} ä¸ªchunks")
        
    except Exception as e:
        print(f"  âŒ å¤±è´¥: {e}")
    
    # æµ‹è¯•è§„ç« åˆ¶åº¦åˆ‡åˆ†å™¨ç±»
    print("\nğŸ“‹ RegulationSplitter:")
    try:
        regulation_splitter = RegulationSplitter(
            regulation_type="finance",
            max_tokens=1800,
            strict_max_tokens=True
        )
        regulation_chunks = regulation_splitter.split_regulation(test_file)
        print(f"  è´¢åŠ¡è§„ç« åˆ‡åˆ†: {len(regulation_chunks)} ä¸ªchunks")
        
    except Exception as e:
        print(f"  âŒ å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æµ‹è¯•æ–°å¢æ”¹è¿›åŠŸèƒ½")
    print("=" * 80)
    
    test_text_cleaning()
    print("\n" + "=" * 80)
    
    test_strict_max_tokens()
    print("\n" + "=" * 80)
    
    test_domain_helpers()
    print("\n" + "=" * 80)
    
    test_domain_splitter_classes()
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æµ‹è¯•å®Œæˆ")
    print("\nğŸ’¡ æ–°åŠŸèƒ½è¯´æ˜:")
    print("1. æ–‡æœ¬æ¸…ç†ï¼šå»é™¤å­—é—´å¤šä½™ç©ºæ ¼ï¼Œä¿ç•™æ¢è¡Œç¬¦")
    print("2. ä¸¥æ ¼chunkæ§åˆ¶ï¼šå¯é€‰æ‹©ä¸¥æ ¼æ§åˆ¶chunkå¤§å°ï¼Œè¶…è¿‡é™åˆ¶æ—¶è‡ªåŠ¨åˆ†å‰²")
    print("3. ä¸“ä¸šé¢†åŸŸhelperï¼šé’ˆå¯¹æ³•å¾‹ã€åˆåŒã€è§„ç« åˆ¶åº¦çš„ä¸“é—¨åˆ‡åˆ†å™¨")
    print("4. æ™ºèƒ½å‚æ•°é…ç½®ï¼šæ ¹æ®æ–‡æ¡£ç±»å‹è‡ªåŠ¨è°ƒæ•´åˆ‡åˆ†å‚æ•°")
    print("5. è¯­ä¹‰å®Œæ•´æ€§ï¼šä¿æŒä¸“ä¸šæ–‡æ¡£çš„è¯­ä¹‰è¿è´¯æ€§")


if __name__ == "__main__":
    main()
